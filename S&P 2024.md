[Toc]
- [**联邦学习**](#--------)
  * [**Protecting Label Distribution in Cross-Silo Federated Learning**](#--protecting-label-distribution-in-cross-silo-federated-learning--)
  * [**FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks**](#--flshield--a-validation-based-federated-learning-framework-to-defend-against-poisoning-attacks--)
  * [**SHERPA: Explainable Robust Algorithms for Privacy-preserved Federated Learning in Future Networks to Defend against Data Poisoning Attacks**](#--sherpa--explainable-robust-algorithms-for-privacy-preserved-federated-learning-in-future-networks-to-defend-against-data-poisoning-attacks--)
  * [**Attacking Byzantine Robust Aggregation in High Dimensions**](#--attacking-byzantine-robust-aggregation-in-high-dimensions--)
  * [**LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation**](#--loki--large-scale-data-reconstruction-attack-against-federated-learning-through-model-manipulation--)
  * [**BadVFL: Backdoor Attacks in Vertical Federated Learning**](#--badvfl--backdoor-attacks-in-vertical-federated-learning--)
- [**区块链**](#-------)
  * [**Specular: Towards Secure, Trust-minimized Optimistic Blockchain Execution**](#--specular--towards-secure--trust-minimized-optimistic-blockchain-execution--)
  * [**Nyx: Detecting Exploitable Front-Running Vulnerabilities in Smart Contracts**](#--nyx--detecting-exploitable-front-running-vulnerabilities-in-smart-contracts--)
  * [**NURGLE: Exacerbating Resource Consumption in Blockchain State Storage via MPT Manipulation**](#--nurgle--exacerbating-resource-consumption-in-blockchain-state-storage-via-mpt-manipulation--)
  * [**SMARTINV: Multimodal Learning for Smart Contract Invariant Inference**](#--smartinv--multimodal-learning-for-smart-contract-invariant-inference--)
  * [**SoK: Security and Privacy of Blockchain Interoperability**](#--sok--security-and-privacy-of-blockchain-interoperability--)
  * [**Pulling Off The Mask: Forensic Analysis of the Deceptive Creator Wallets Behind Smart Contract Fraud**](#--pulling-off-the-mask--forensic-analysis-of-the-deceptive-creator-wallets-behind-smart-contract-fraud--)
  * [**Towards Smart Contract Fuzzing on GPU**](#--towards-smart-contract-fuzzing-on-gpu--)
  * [**Routing Attacks on Cryptocurrency Mining Pools**](#--routing-attacks-on-cryptocurrency-mining-pools--)
  * [**Larger-scale Nakamoto-style Blockchains Don't Necessarily Offer Better Security**](#--larger-scale-nakamoto-style-blockchains-don-t-necessarily-offer-better-security--)
  * [**Large-Scale Study of Vulnerability Scanners for Ethereum Smart Contracts**](#--large-scale-study-of-vulnerability-scanners-for-ethereum-smart-contracts--)
- [**安全多方计算**](#----------)
  * [**GAuV: A Graph-Based Automated Verification Framework for Perfect Semi-Honest Security of Multiparty Computation Protocols**](#--gauv--a-graph-based-automated-verification-framework-for-perfect-semi-honest-security-of-multiparty-computation-protocols--)
  * [**Don't Eject the Impostor: Fast Three-Party Computation With a Known Cheater**](#--don-t-eject-the-impostor--fast-three-party-computation-with-a-known-cheater--)
  * [**MPC-in-the-Head Framework without Repetition and its Applications to the Lattice-based Cryptography**](#--mpc-in-the-head-framework-without-repetition-and-its-applications-to-the-lattice-based-cryptography--)
  * [**Scalable Mixed-Mode MPC**](#--scalable-mixed-mode-mpc--)
  * [**Asterisk: Super-fast MPC with a Friend**](#--asterisk--super-fast-mpc-with-a-friend--)
  * [**Efficient Actively Secure DPF and RAM-based 2PC with One-Bit Leakage**](#--efficient-actively-secure-dpf-and-ram-based-2pc-with-one-bit-leakage--)
  * [**SoK: Collusion-resistant Multi-party Private Set Intersections in the Semi-honest Model**](#--sok--collusion-resistant-multi-party-private-set-intersections-in-the-semi-honest-model--)
- [**差分隐私**](#--------)
  * [**Eureka: A General Framework for Black-box Differential Privacy Estimators**](#--eureka--a-general-framework-for-black-box-differential-privacy-estimators--)
  * [**Casual Users and Rational Choices within Differential Privacy**](#--casual-users-and-rational-choices-within-differential-privacy--)
  * [**Lower Bounds for Rényi Differential Privacy in a Black-Box Setting**](#--lower-bounds-for-r-nyi-differential-privacy-in-a-black-box-setting--)
  * [**Bounded and Unbiased Composite Differential Privacy**](#--bounded-and-unbiased-composite-differential-privacy--)
  * [**Cohere: Managing Differential Privacy in Large Scale Systems**](#--cohere--managing-differential-privacy-in-large-scale-systems--)
  * [**DPI: Ensuring Strict Differential Privacy for Infinite Data Streaming**](#--dpi--ensuring-strict-differential-privacy-for-infinite-data-streaming--)
  * [**Budget Recycling Differential Privacy**](#--budget-recycling-differential-privacy--)
  * [**Measure-Observe-Remeasure: An Interactive Paradigm for Differentially-Private Exploratory Analysis**](#--measure-observe-remeasure--an-interactive-paradigm-for-differentially-private-exploratory-analysis--)
  * [**DP-Auditorium: a Large Scale Library for Auditing Differential Privacy**](#--dp-auditorium--a-large-scale-library-for-auditing-differential-privacy--)
- [**DoS & DDoS**](#--dos---ddos--)
  * [**DNSBomb: A New Practical-and-Powerful Pulsing DoS Attack Exploiting DNS Queries-and-Responses**](#--dnsbomb--a-new-practical-and-powerful-pulsing-dos-attack-exploiting-dns-queries-and-responses--)
  * [**Leveraging Prefix Structure to Detect Volumetric DDoS Attack Signatures with Programmable Switches**](#--leveraging-prefix-structure-to-detect-volumetric-ddos-attack-signatures-with-programmable-switches--)
- [**零知识证明**](#---------)
  * [**Efficient Zero-Knowledge Arguments For Paillier Cryptosystem**](#--efficient-zero-knowledge-arguments-for-paillier-cryptosystem--)
  * [**Scalable Verification of Zero-Knowledge Protocols**](#--scalable-verification-of-zero-knowledge-protocols--)
  * [**Private Analytics via Streaming, Sketching, and Silently Verifiable Proofs**](#--private-analytics-via-streaming--sketching--and-silently-verifiable-proofs--)
  * [**SwiftRange: A Short and Efficient Zero-Knowledge Range Argument For Confidential Transactions and More**](#--swiftrange--a-short-and-efficient-zero-knowledge-range-argument-for-confidential-transactions-and-more--)
  * [**Certifying Zero-Knowledge Circuits with Refinement Types**](#--certifying-zero-knowledge-circuits-with-refinement-types--)
  * [**Ligetron: Lightweight Scalable End-to-End Zero-Knowledge Proofs. Post-Quantum ZK-SNARKs on a Browser**](#--ligetron--lightweight-scalable-end-to-end-zero-knowledge-proofs-post-quantum-zk-snarks-on-a-browser--)
  * [**Pianist: Scalable zkRollups via Fully Distributed Zero-Knowledge Proofs**](#--pianist--scalable-zkrollups-via-fully-distributed-zero-knowledge-proofs--)
- [**后门攻击**](#--------)
  * [**Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics**](#--robust-backdoor-detection-for-deep-learning-via-topological-evolution-dynamics--)
  * [**ODSCAN: Backdoor Scanning for Object Detection Models**](#--odscan--backdoor-scanning-for-object-detection-models--)
  * [**BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets**](#--baffle--hiding-backdoors-in-offline-reinforcement-learning-datasets--)
  * [**DeepVenom: Persistent DNN Backdoors Exploiting Transient Weight Perturbations in Memories**](#--deepvenom--persistent-dnn-backdoors-exploiting-transient-weight-perturbations-in-memories--)
  * [**Need for Speed: Taming Backdoor Attacks with Speed and Precision**](#--need-for-speed--taming-backdoor-attacks-with-speed-and-precision--)
  * [**Exploring the Orthogonality and Linearity of Backdoor Attacks**](#--exploring-the-orthogonality-and-linearity-of-backdoor-attacks--)
  * [**BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting**](#--belt--old-school-backdoor-attacks-can-evade-the-state-of-the-art-defense-with-backdoor-exclusivity-lifting--)
  * [**MMBD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic**](#--mmbd--post-training-detection-of-backdoor-attacks-with-arbitrary-backdoor-pattern-types-using-a-maximum-margin-statistic--)
  * [**Distribution Preserving Backdoor Attack in Self-supervised Learning**](#--distribution-preserving-backdoor-attack-in-self-supervised-learning--)
  * [**Backdooring Multimodal Learning**](#--backdooring-multimodal-learning--)
- [**攻击与检测**](#---------)
  * [**Please Tell Me More: Privacy Impact of Explainability through the Lens of Membership Inference Attack**](#--please-tell-me-more--privacy-impact-of-explainability-through-the-lens-of-membership-inference-attack--)
  * [**MEA-Defender: A Robust Watermark against Model Extraction Attack**](#--mea-defender--a-robust-watermark-against-model-extraction-attack--)
  * [**Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models**](#--poisoned-chatgpt-finds-work-for-idle-hands--exploring-developers--coding-practices-with-insecure-suggestions-from-poisoned-ai-models--)
  * [**Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models**](#--nightshade--prompt-specific-poisoning-attacks-on-text-to-image-generative-models--)
  * [**Dropout Attacks**](#--dropout-attacks--)
  * [**Test-Time Poisoning Attacks Against Test-Time Adaptation Models**](#--test-time-poisoning-attacks-against-test-time-adaptation-models--)
- [**其他**](#------)
  * [**Few-shot Unlearning**](#--few-shot-unlearning--)
  * [**Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning**](#--learn-what-you-want-to-unlearn--unlearning-inversion-attacks-against-machine-unlearning--)
  * [**Distributed & Scalable Oblivious Sorting and Shuffling**](#--distributed---scalable-oblivious-sorting-and-shuffling--)
  * [**SoK: Unintended Interactions among Machine Learning Defenses and Risks**](#--sok--unintended-interactions-among-machine-learning-defenses-and-risks--)
  * [**SoK: Technical Implementation and Human Impact of Internet Privacy Regulations**](#--sok--technical-implementation-and-human-impact-of-internet-privacy-regulations--)
  * [**Private Hierarchical Governance for Encrypted Messaging**](#--private-hierarchical-governance-for-encrypted-messaging--)
  * [**From Virtual Touch to Tesla Command: Unlocking Unauthenticated Control Chains From Smart Glasses for Vehicle Takeover**](#--from-virtual-touch-to-tesla-command--unlocking-unauthenticated-control-chains-from-smart-glasses-for-vehicle-takeover--)
  * [**From Chatbots to Phishbots?: Phishing Scam Generation in Commercial Large Language Models**](#--from-chatbots-to-phishbots---phishing-scam-generation-in-commercial-large-language-models--)
  * [**SoK: Explainable Machine Learning in Adversarial Environments**](#--sok--explainable-machine-learning-in-adversarial-environments--)
  * [**Secure Messaging with Strong Compromise Resilience, Temporal Privacy, and Immediate Decryption**](#--secure-messaging-with-strong-compromise-resilience--temporal-privacy--and-immediate-decryption--)
  * [**Don't Shoot the Messenger: Localization Prevention of Satellite Internet Users**](#--don-t-shoot-the-messenger--localization-prevention-of-satellite-internet-users--)
  * [**SoK: Privacy-Preserving Data Synthesis**](#--sok--privacy-preserving-data-synthesis--)



# **联邦学习**

## **Protecting Label Distribution in Cross-Silo Federated Learning**

Yangfan Jiang (National University of Singapore), Xinjian Luo (National University of Singapore), Yuncheng Wu (National University of Singapore), Xiaokui Xiao (National University of Singapore), Beng Chin Ooi (National University of Singapore)

**Abstract:** Federated learning (FL) is a popular distributed machine learning (ML) framework in which multiple parties share their model parameters instead of the raw training datasets to construct a global model in a privacy-preserving manner. However, existing FL solutions mainly focus on protecting the privacy of individual training records by incorporating differential privacy (DP), while overlooking the protection of the distribution information of training datasets, despite the fact that data distribution is also regarded as highly sensitive in high-stakes applications. In this paper, we propose the **first privacy-preserving stochastic gradient descent (SGD) algorithm** for protecting label distribution in FL. To establish a formal privacy guarantee, we formalize a privacy notion, dubbed (m,γ,ξ)-label distributional privacy, to quantify label distributional privacy leakage. Subsequently, we design the label distribution perturbation mechanism (LDPM) that carefully incorporates randomness into the SGD algorithm to achieve (m,γ,ξ)-label distributional privacy for all one-vs-all classification models. LDPM is easy to implement and provides non-trivial privacy guarantees, making it a suitable drop-in replacement for existing FL local model training algorithms. Notably, we demonstrate that LDPM also ensures DP, indicating that LDPM offers both individual and label distributional privacy guarantees. Extensive experiments on six benchmark datasets validate the effectiveness of LDPM.



## **FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks**

Ehsanul Kabir (Penn State University), Zeyu Song (Penn State University), Md Rafi Ur Rashid (Penn State University), Shagufta Mehnaz (Penn State University)

**Abstract:** Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants’ actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server’s access to clean datasets—an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.



## **SHERPA: Explainable Robust Algorithms for Privacy-preserved Federated Learning in Future Networks to Defend against Data Poisoning Attacks**

Chamara Sandeepa (University College Dublin), Bartlomiej Siniarski (University College Dublin), Shen Wang (University College Dublin), Madhusanka Liyanage (University College Dublin)

**Abstract:** With the rapid progression of communication and localisation of big data over billions of devices, distributed Machine Learning (ML) techniques are emerging to cater for the development of Artificial Intelligence (AI)-based services in a distributed manner. Federated Learning (FL) is such an innovative approach to achieve a privacy-preserved AI that facilitates ML model sharing and aggregation while keeping the participants’ data at the original source. However, recent research has investigated threats from poisoning attacks in FL. Several robust algorithms based on techniques such as similarity metrics or anomaly filtering are proposed as solutions. Yet, these approaches do not focus on investigating the intentions of the attackers or providing justifications and evidence for suspecting the behaviour of clients who are considered poisoners. Therefore, we propose SHERPA, a robust algorithm that uses Shapley Additive Explanations (SHAP) to identify potential poisoners in an FL system. Based on this, we develop a novel algorithm to differentiate poisoners via feature attribution clustering. We launch data poisoning attacks for different scenarios on multiple datasets and showcase our solution to mitigate the attacks. Furthermore, we show that privacy-targeted poisoning attacks can be mitigated with our approach. Accompanying the Explainable AI (XAI) technique for defence, our study reveals the potential for post-hoc feature attributions in countering data poisoning attacks with better explainability and improved justification in eliminating potentially malicious clients in the aggregation process.



## **Attacking Byzantine Robust Aggregation in High Dimensions**

Sarthak Choudhary (National University of Sinagpore), Aashish Kolluri (National University of Singapore), Prateek Saxena (National University of Singapore)

**Abstract:** Training modern neural networks or models typically requires averaging over a sample of high-dimensional vectors. Poisoning attacks can skew or bias the average vectors used to train the model, forcing the model to learn specific patterns or avoid learning anything useful. Byzantine robust aggregation is a principled algorithmic defense against such biasing. Robust aggregators can bound the maximum bias in computing centrality statistics, such as mean, even when some fraction of inputs are arbitrarily corrupted. Designing such aggregators is challenging when dealing with high dimensions. However, the first polynomial-time algorithms with strong theoretical bounds on the bias have recently been proposed. Their bounds are independent of the number of dimensions, promising a conceptual limit on the power of poisoning attacks in their ongoing arms race against defenses.In this paper, we show a new attack called HiDRA on practical realization of strong defenses which subverts their claim of dimension-independent bias. HiDRA highlights a novel computational bottleneck that has not been a concern of prior information-theoretic analysis. Our experimental evaluation shows that our attacks almost completely destroy the model performance, whereas existing attacks with the same goal fail to have much effect. Our findings leave the arms race between poisoning attacks and provable defenses wide open.



## **LOKI: Large-scale Data Reconstruction Attack against Federated Learning through Model Manipulation**

Joshua C. Zhao (Purdue University), Atul Sharma (Purdue University), Ahmed Roushdy Elkordy (University of Southern California), Yahya H. Ezzeldin (University of Southern California), Salman Avestimehr (University of Southern California), Saurabh Bagchi (Purdue University)

**Abstract:** Federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. Despite this, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients.However, prior data reconstruction attacks have been limited in setting and scale, as most works target FedSGD and limit the attack to single-client gradients. Many of these attacks fail in the more practical setting of FedAVG or if updates are aggregated together using secure aggregation. Data reconstruction becomes significantly more difficult, resulting in limited attack scale and/or decreased reconstruction quality. When both FedAVG and secure aggregation are used, there is no current method that is able to attack multiple clients concurrently in a federated learning setting.In this work we introduce Loki, an attack that overcomes previous limitations and also breaks the anonymity of aggregation as the leaked data is identifiable and directly tied back to the clients they come from. Our design sends clients customized convolutional parameters, and the weight gradients of data points between clients remain separate even through aggregation. With FedAVG and aggregation across 100 clients, prior work can leak less than 1% of images on MNIST, CIFAR-100, and Tiny ImageNet. Using only a single training round, Loki is able to leak 76-86% of all data samples.



## **BadVFL: Backdoor Attacks in Vertical Federated Learning**

Mohammad Naseri (University College London), Yufei Han (Inria Rennes), Emiliano De Cristofaro (University College London)

Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attacks in VFL is more challenging than in HFL, as the adversary i) does not have access to the labels during training and ii) cannot change the labels as she only has access to the feature embeddings. We present a first-of-its-kind clean-label backdoor attack in VFL, which consists of two phases: a label inference and a backdoor phase. We demonstrate the effectiveness of the attack on three different datasets, investigate the factors involved in its success, and discuss countermeasures to mitigate its impact.



# **区块链**

## **Specular: Towards Secure, Trust-minimized Optimistic Blockchain Execution**

Zhe Ye (UC Berkeley), Ujval Misra (UC Berkeley), Jiajun Cheng (ShanghaiTech University), Andy Zhou (Cambridge University), Dawn Song (UC Berkeley)



## **Nyx: Detecting Exploitable Front-Running Vulnerabilities in Smart Contracts**

Wuqi Zhang (The Hong Kong University of Science and Technology), Zhuo Zhang (Purdue University), Qingkai Shi (Purdue University), Lu Liu (The Hong Kong University of Science and Technology), Lili Wei (McGill University), Yepang Liu (Southern University of Science and Technology), Xiangyu Zhang (Purdue University), Shing-Chi Cheung (The Hong Kong University of Science and Technology)



## **NURGLE: Exacerbating Resource Consumption in Blockchain State Storage via MPT Manipulation**

Zheyuan He (University of Electronic Science and Technology of China), Zihao Li (The Hong Kong Polytechnic University), Ao Qiao (University of Electronic Science and Technology of China), Xiapu Luo (The Hong Kong Polytechnic University), Xiaosong Zhang (University of Electronic Science and Technology of China), Ting Chen (University of Electronic Science and Technology of China), Shuwei Song (University of Electronic Science and Technology of China), Dijun Liu (Ant Group), Weina Niu (University of Electronic Science and Technology of China)



## **SMARTINV: Multimodal Learning for Smart Contract Invariant Inference**

Sally Junsong Wang (Columbia University), Kexin Pei (Columbia University), Junfeng Yang (Columbia University), Sally Junsong Wang (Columbia University)



## **SoK: Security and Privacy of Blockchain Interoperability**

Andre Augusto (INESC-ID & Instituto Superior Técnico), Rafael Belchior (INESC-ID & Instituto Superior Técnico), Miguel Nuno Dias Alves Pupo Correia (INESC-ID & Instituto Superior Técnico), Andre Vasconcelos (INESC-ID & Instituto Superior Técnico), Luyao Zhang (Duke Kunshan University), Thomas Hardjono (MIT Connection Science)



## **Pulling Off The Mask: Forensic Analysis of the Deceptive Creator Wallets Behind Smart Contract Fraud**

Mingxuan Yao (Georgia Institute of Technology), Runze Zhang (Georgia Institute of Technology), Haichuan Xu (Georgia Institute of Technology), Ryan Chou (Georgia Institute of Technology), Varun Chowdhary Paturi (Georgia Institute of Technology), Amit Kumar Sikder (Georgia Institute of Technology), Brendan Saltaformaggio (Georgia Tech)



## **Towards Smart Contract Fuzzing on GPU**

Weimin Chen (The Hong Kong Polytechnic University), Xiapu Luo (The Hong Kong Polytechnic University), Haipeng Cai (Washington State University), Haoyu Wang (Huazhong University of Science and Technology)



## **Routing Attacks on Cryptocurrency Mining Pools**

Muoi Tran (ETH Zürich), Theo von Arx (ETH Zürich), Laurent Vanbever (ETH Zürich)



## **Larger-scale Nakamoto-style Blockchains Don't Necessarily Offer Better Security**

Jannik Albrecht (Ruhr University Bochum), Sebastien Andreina (NEC Laboratories Europe), Frederik Armknecht (University of Mannheim), Ghassan Karame (Ruhr-University Bochum), Giorgia Marson (NEC Laboratories Europe), Julian Willingmann (Ruhr-University Bochum)



## **Large-Scale Study of Vulnerability Scanners for Ethereum Smart Contracts**

Christoph Sendner (University of Würzburg), Lukas Petzi (University of Würzburg), Jasper Stang (University of Würzburg), Alexandra Dmitrienko (University of Würzburg)



# **安全多方计算**

## **GAuV: A Graph-Based Automated Verification Framework for Perfect Semi-Honest Security of Multiparty Computation Protocols**

Xingyu Xie (Tsinghua University; RealAI), Yifei Li (Tsinghua University), Wei Zhang (Tsinghua University), Tuowei Wang (Tsinghua University), Shizhen Xu (RealAI), Jun Zhu (Tsinghua University; RealAI), Yifan Song (Tsinghua University)



## **Don't Eject the Impostor: Fast Three-Party Computation With a Known Cheater**

Andreas Brüggemann (Technical University of Darmstadt, Germany), Oliver Schick (Technical University of Darmstadt, Germany), Thomas Schneider (Technical University of Darmstadt, Germany), Ajith Suresh (Technology Innovation Institute, Abu Dhabi), Hossein Yalame (Technical University of Darmstadt, Germany)



## **MPC-in-the-Head Framework without Repetition and its Applications to the Lattice-based Cryptography**

Weihao Bai (Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences), Long Chen (Institute of Software, Chinese Academy of Sciences), Qianwen Gao (Institute of Software, Chinese Academy of Sciences; University of Chinese Academy of Sciences), Zhenfeng Zhang (Institute of Software, Chinese Academy of Sciences)



## **Scalable Mixed-Mode MPC**

Radhika (Northwestern University), Kang Yang (State Key Laboratory of Cryptology), Jonathan Katz (University of Maryland), Xiao Wang (Northwestern University)



## **Asterisk: Super-fast MPC with a Friend**

Banashri Karmakar (Indian Institute of Science, Bangalore), Nishat Koti (Indian Institute of Science, Bangalore), Arpita Patra (Indian Institute of Science, Bangalore), Sikhar Patranabis (IBM Research - India), Protik Paul (Indian Institute of Science, Bangalore), Divya Ravi (Aarhus University), Sikhar Patranabis (IBM Research India)



## **Efficient Actively Secure DPF and RAM-based 2PC with One-Bit Leakage**

Wenhao Zhang (Northwestern University), Xiaojie Guo (Nankai University, State Key Laboratory of Cryptology), Kang Yang (State Key Laboratory of Cryptology), Ruiyu Zhu (No Affiliation), Yu Yu (Shanghai Jiao Tong University, Shanghai Qi Zhi Institute), Xiao Wang (Northwestern University)



## **SoK: Collusion-resistant Multi-party Private Set Intersections in the Semi-honest Model**

Jelle Vos (Delft University of Technology), Mauro Conti (University of Padua), Zekeriya Erkin (Delft University of Technology)



# **差分隐私**

## **Eureka: A General Framework for Black-box Differential Privacy Estimators**

Yun Lu (University of Victoria), Malik Magdon-Ismail (Rensselaer Polytechnic Institute), Yu Wei (Purdue University), Vassilis Zikas (Purdue University)



## **Casual Users and Rational Choices within Differential Privacy**

Narges Ashena (University of Zurich), Oana Inel (University of Zurich), Badrie L. Persaud (UBS), Abraham Bernstein (University of Zurich)



## **Lower Bounds for Rényi Differential Privacy in a Black-Box Setting**

Tim Kutta (Ruhr-University Bochum), à–nder Askin (Ruhr-University Bochum), Martin Dunsche (Ruhr-University Bochum)



## **Bounded and Unbiased Composite Differential Privacy**

Kai Zhang (Swinburne University of Technology), Yanjun Zhang (University of Technology Sydney and CSIRO's Data61), Ruoxi Sun (CSIRO's Data61), Pei-Wei Tsai (Swinburne University of Technology), Muneeb Ul Hassan (Deakin University), Xin Yuan (CSIRO's Data61), Minhui Xue (CSIRO's Data61), Jinjun Chen (Swinburne University of Technology)



## **Cohere: Managing Differential Privacy in Large Scale Systems**

Nicolas Küchler (ETH Zurich), Emanuel Opel (ETH Zurich), Hidde Lycklama (ETH Zurich), Alexander Viand (Intel Labs), Anwar Hithnawi (ETH Zurich)



## **DPI: Ensuring Strict Differential Privacy for Infinite Data Streaming**

Shuya Feng (University of Connecticut), Meisam Mohammady (Iowa State University), Han Wang (University of Kansas), Xiaochen Li (Zhejiang University), Zhan Qin (Zhejiang University), Yuan Hong (University of Connecticut)



## **Budget Recycling Differential Privacy**

Bo Jiang (TikTok Inc.), Jian Du (TikTok Inc.), Sagar Sharma (TikTok Inc.), Qiang Yan (TikTok Inc.)



## **Measure-Observe-Remeasure: An Interactive Paradigm for Differentially-Private Exploratory Analysis**

Priyanka Nanayakkara (Northwestern University), Hyeok Kim (Northwestern University), Yifan Wu (Northwestern University), Ali Sarvghad (UMass Amherst), Narges Mahyar (UMass Amherst), Gerome Miklau (UMass Amherst), Jessica Hullman (Northwestern University)



## **DP-Auditorium: a Large Scale Library for Auditing Differential Privacy**

William Kong (Google), Andres Muñoz Medina (Google), Monica Ribero (Google), Umar Syed (Google)



# **DoS & DDoS**



## **DNSBomb: A New Practical-and-Powerful Pulsing DoS Attack Exploiting DNS Queries-and-Responses**

Xiang Li (Tsinghua University), Dashuai Wu (Tsinghua University), Haixin Duan (Tsinghua University), Qi Li (Tsinghua University)

**Abstract:** DNS employs a variety of mechanisms to guarantee availability, protect security, and enhance reliability. In this paper, however, we reveal that these inherent beneficial mechanisms, including timeout, query aggregation, and response fastreturning, can be transformed into malicious attack vectors. We propose a new practical and powerful pulsing DoS attack, dubbed the DNSBOMB attack. DNSBOMB exploits multiple widely-implemented DNS mechanisms to accumulate DNS queries that are sent at a low rate, amplify queries into large-sized responses, and concentrate all DNS responses into a short, high-volume periodic pulsing burst to simultaneously overwhelm target systems. Through an extensive evaluation on 10 mainstream DNS software, 46 public DNS services, and around 1.8 M open DNS resolvers, we demonstrate all DNS resolvers could be exploited to conduct more practical-andpowerful DNSBOMB attacks than previous pulsing DoS attacks. Small-scale experiments show the peak pulse magnitude can approach 8.7 Gb/s and the bandwidth amplification factor could exceed 20,000 x. Our controlled attacks cause complete packet loss or service degradation on both stateless and stateful connections (TCP, UDP, and QUIC). In addition, we present effective mitigation solutions with detailed evaluations. We have responsibly reported our findings to all affected vendors, and received acknowledgement from 24 of them, which are patching their software using our solutions, such as BIND, Unbound, PowerDNS, and Knot. 10 CVE-IDs are assigned.



## **Leveraging Prefix Structure to Detect Volumetric DDoS Attack Signatures with Programmable Switches**

Chris Misa (University of Oregon), Ram Durairajan (University of Oregon), Arpit Gupta (UCSB), Reza Rejaie (University of Oregon), Walter Willinger (NIKSUN, Inc.)

**Abstract:** As increasingly complex and dynamic volumetric DDoS attacks continue to wreak havoc on edge networks, two recent developments promise to bolster DDoS defense at the edge. First, p**rogrammable switches have emerged as promising means for achieving scalable and cost-effective attack signature detection**. However, their practical application in edge networks remains a challenging open problem. Second, **machine learning (ML)-based solutions have demonstrated potential in accurately detecting attack signatures based on per-flow traffic features**. Yet, their inability to effectively scale to the traffic volumes and number of flows in actual production edge networks has largely excluded them from practical considerations. In this paper, we introduce ZAPDOS, a novel approach to accurately, quickly, and scalably detect volumetric DDoS attack signatures at the source prefix level. ZAPDOS is the first to utilize a key characteristic of the observed structure of measured attack and benign source prefixes (i.e., a pronounced cluster-within-cluster property) and effectively apply it in practice against modern attacks. ZAPDOS operates by monitoring aggregate prefix-level features in switch hardware, employing a learning model to identify prefixes suspected of containing attack sources, and using several innovative algorithmic methods to pinpoint attack sources efficiently. We have built a hardware prototype of ZAPDOS and a packet-level software simulator which achieve comparable accuracy results. Since existing datasets are inadequate for training and evaluating prefix-level models, we have developed a new data-fusion methodology for training and evaluating ZAPDOS. We use our prototype and simulator to show that ZAPDOS can detect volumetric DDoS attack signatures with orders of magnitude lower error rates than state-of-the-art under comparable monitoring resource budgets and for a range of different attack scenarios.



# **零知识证明**

## **Efficient Zero-Knowledge Arguments For Paillier Cryptosystem**

Borui Gong (The Hong Kong Polytechnic University), Wang Fat Lau (The Hong Kong Polytechnic University), Man Ho Au (The Hong Kong Polytechnic University), Rupeng Yang (University of Wollongong), Haiyang Xue (The Hong Kong Polytechnic University), Lichun Li (Ant Group)



## **Scalable Verification of Zero-Knowledge Protocols**

Miguel Isabel (Universidad Complutense de Madrid), Clara Rodrà­guez-Nàºñez (Universidad Complutense de Madrid), Albert Rubio (Universidad Complutense de Madrid)



## **Private Analytics via Streaming, Sketching, and Silently Verifiable Proofs**

Mayank Rathee (UC Berkeley), Yuwen Zhang (UC Berkeley), Henry Corrigan-Gibbs (MIT), Raluca Ada Popa (UC Berkeley)



## **SwiftRange: A Short and Efficient Zero-Knowledge Range Argument For Confidential Transactions and More**

Nan Wang (Australian National University and CSIRO's Data61), Sid Chi-Kin Chau (Australian National University and CSIRO's Data61), DongXi Liu (CSIRO's Data61), Nan Wang (Australian National University), Sid Chi-Kin Chau (Australian National University)



## **Certifying Zero-Knowledge Circuits with Refinement Types**

Junrui Liu (University of California, Santa Barbara), Ian Kretz (The University of Texas at Austin), Hanzhi Liu (University of California, Santa Barbara / Veridise Inc.), Bryan Tan (Veridise Inc.), Jonathan Wang (Axiom), Yi Sun (Axiom), Luke Pearson (Polychain Capital), Anders Miltner (Simon Fraser University), IÅŸÄ±l Dillig (The University of Texas at Austin / Veridise Inc.), Yu Feng (University of California, Santa Barbara / Veridise Inc.)



## **Ligetron: Lightweight Scalable End-to-End Zero-Knowledge Proofs. Post-Quantum ZK-SNARKs on a Browser**

Carmit Hazay (Ligero Inc.), Muthuramakrishnan Venkitasubramaniam (Ligero Inc.), Ruihan Wang (Ligero Inc.)



## **Pianist: Scalable zkRollups via Fully Distributed Zero-Knowledge Proofs**

Tianyi Liu (University of Illinois Urbana-Champaign), Tiancheng Xie (UC Berkeley), Jiaheng Zhang (UC Berkeley), Dawn Song (UC Berkeley), Yupeng Zhang (University of Illinois Urbana-Champaign), Tianyi Liu (Texas A&M University), Yupeng Zhang (Texas A&M University)



# **后门攻击**

## **Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics**

Xiaoxing Mo (Deakin University), Yechao Zhang (Huazhong University of Science and Technology), Leo Yu Zhang (Griffith University), Wei Luo (Deakin University), Nan Sun (University of New South Wales Canberra), Shengshan Hu (Huazhong University of Science and Technology), Shang Gao (Deakin University), Yang Xiang (Swinburne University of Technology)



## **ODSCAN: Backdoor Scanning for Object Detection Models**

Siyuan Cheng (Purdue University), Guangyu Shen (Purdue University), Guanhong Tao (Purdue University), Kaiyuan Zhang (Purdue University), Zhuo Zhang (Purdue University), Shengwei An (Purdue University), Xiangzhe Xu (Purdue University), Yingqi Liu (Purdue University), Shiqing Ma (University of Massachusetts, Amherst), Xiangyu Zhang (Purdue University)



## **BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets**

Chen Gong (University of Virginia), Zhou Yang (Singapore Management University), Yunpeng Bai (Institute of Automation, Chinese Academy of Sciences), Jieke Shi (Singapore Management University), Junda He (Singapore Management University), Kecen Li (Institute of Automation, Chinese Academy of Sciences), Bowen Xu (North Carolina State University), Arunesh Sinha (Rutgers University), Xinwen Hou (Institute of Automation, Chinese Academy of Sciences), David Lo (Singapore Management University), Tianhao Wang (University of Virginia)



## **DeepVenom: Persistent DNN Backdoors Exploiting Transient Weight Perturbations in Memories**

Kunbei Cai (University of Central Florida), Md Hafizul Islam Chowdhuryy (University of Central Florida), Zhenkai Zhang (Clemson University), Fan Yao (University of Central Florida)



## **Need for Speed: Taming Backdoor Attacks with Speed and Precision**

Zhuo Ma (Xidian University), Yilong Yang (Xidian University), Yang Liu (Xidian University), Tong Yang (Peking University), Xinjing Liu (Xidian University), Teng Li (Xidian University), Zhan Qin (Zhejiang University)、



## **Exploring the Orthogonality and Linearity of Backdoor Attacks**

Kaiyuan Zhang (Purdue University), Siyuan Cheng (Purdue University), Guangyu Shen (Purdue University), Guanhong Tao (Purdue University), Shengwei An (Purdue University), Anuran Makur (Purdue University), Shiqing Ma (UMass Amherst), Xiangyu Zhang (Purdue University)



## **BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting**

Huming Qiu (Fudan University), Junjie Sun (Fudan University), Mi Zhang (Fudan University), Xudong Pan (Fudan University), Min Yang (Fudan University)



## **MMBD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic**

Hang Wang (Pennsylvania State University), Zhen Xiang (University of Illinois Urbana-Champaign), David J. Miller (Pennsylvania State University), George Kesidis (Pennsylvania State University)



## **Distribution Preserving Backdoor Attack in Self-supervised Learning**

Guanhong Tao (Purdue University), Zhenting Wang (Rutgers University), Shiwei Feng (Purdue University), Guangyu Shen (Purdue University), Shiqing Ma (Rutgers University), Xiangyu Zhang (Purdue University)



## **Backdooring Multimodal Learning**

Xingshuo Han (Nanyang Technological University), Yutong Wu (Nanyang Technological University), Qingjie Zhang (Shanghai Jiao Tong University), Yuan Zhou (Nanyang Technological University), Yuan Xu (Nanyang Technological University), Han Qiu (Tsinghua University), Guowen Xu (Nanyang Technological University), Tianwei Zhang (Nanyang Technological University)



# **攻击与检测**

## **Please Tell Me More: Privacy Impact of Explainability through the Lens of Membership Inference Attack**

Han Liu (Washington University in St. Louis), Yuhao Wu (Washington University in St. Louis), Zhiyuan Yu (Washington University in St. Louis), Ning Zhang (Washington University in St. Louis)



## **MEA-Defender: A Robust Watermark against Model Extraction Attack**

Peizhuo Lv (Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China), Hualong Ma (Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China), Kai Chen (Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China), Jiachen Zhou (Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China), Shengzhi Zhang (Department of Computer Science, Metropolitan College, Boston University, USA), Ruigang Liang (Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China), Shenchen Zhu (Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China), Pan Li (Institute of Information Engineering, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China), Yingjun Zhang (Institute of Software, Chinese Academy of Sciences, China), Peizhuo Lv (Institute of Information Engineering, Chinese Academy of Sciences)



## **Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models**

Sanghak Oh (Sungkyunkwan University), Kiho Lee (Sungkyunkwan University), Seonhye Park (Sungkyunkwan University), Doowon Kim (University of Tennessee), Hyoungshick Kim (Sungkyunkwan University)



## **Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models**

Shawn Shan (University of Chicago), Wenxin Ding (University of Chicago), Josephine Passananti (University of Chicago), Stanley Wu (University of Chicago), Haitao Zheng (University of Chicago), Ben Y. Zhao (University of Chicago)



## **Dropout Attacks**

Andrew Yuan (Northeastern University), Alina Oprea (Northeastern University), Cheng Tan (Northeastern University)



## **Test-Time Poisoning Attacks Against Test-Time Adaptation Models**

Tianshuo Cong (Tsinghua University), Xinlei He (CISPA Helmholtz Center for Information Security), Yun Shen (NetApp), Yang Zhang (CISPA Helmholtz Center for Information Security)



# **其他**

## **Few-shot Unlearning**

Youngsik Yoon (POSTECH), Jinhwan Nam (POSTECH), Hyojeong Yun (POSTECH), Jaeho Lee (POSTECH), Dongwoo Kim (POSTECH), Jungseul Ok (POSTECH)



## **Learn What You Want to Unlearn: Unlearning Inversion Attacks against Machine Unlearning**

Hongsheng Hu (CSIRO's Data61), Shuo Wang (Shanghai Jiao Tong University), Tian Dong (Shanghai Jiao Tong University), Minhui Xue (CSIRO's Data61)



## **Distributed & Scalable Oblivious Sorting and Shuffling**

Nicholas Ngai (University of California, Berkeley), Ioannis Demertzis (University of California, Santa Cruz), Javad Ghareh Chamani (Hong Kong University of Science and Technology), Dimitrios Papadopoulos (The Hong Kong University of Science and Technology)



## **SoK: Unintended Interactions among Machine Learning Defenses and Risks**

Vasisht Duddu (University of Waterloo), Sebastian Szyller (Intel Labs), N. Asokan (University of Waterloo, Aalto University)



## **SoK: Technical Implementation and Human Impact of Internet Privacy Regulations**

Eleanor Birrell (Pomona College), Jay Rodolitz (Northeastern University), Angel Ding (Wellesley College), Jenna Lee (University of Washington), Emily McReynolds (Sony AI), Jevan Hutson (Hintze Law PLLC), Ada Lerner (Northeaster University)



## **Private Hierarchical Governance for Encrypted Messaging**

Armin Namavari (Cornell Tech), Barry Wang (Cornell University), Sanketh Menda (Cornell Tech), Ben Nassi (Cornell Tech), Nirvan Tyagi (Stanford University and University of Washington), James Grimmelmann (Cornell University), Amy Zhang (University of Washington), Thomas Ristenpart (Cornell Tech)



## **From Virtual Touch to Tesla Command: Unlocking Unauthenticated Control Chains From Smart Glasses for Vehicle Takeover**

Xingli Zhang (University of Louisiana at Lafayette), Yazhou Tu (University of Louisiana at Lafayette), Yan Long (University of Michigan), Liqun Shan (University of Louisiana at Lafayette), Mohamed A Elsaadani (University of Louisiana at Lafayette), Kevin Fu (Northeastern University), Zhiqiang Lin (Ohio State University), Xiali Hei (University of Louisiana at Lafayette)



## **From Chatbots to Phishbots?: Phishing Scam Generation in Commercial Large Language Models**

Sayak Saha Roy (University of Texas at Arlington), Poojitha Thota (University of Texas at Arlington), Krishna Vamsi Naragam (University of Texas at Arlington), Shirin Nilizadeh (University of Texas at Arlington)



## **SoK: Explainable Machine Learning in Adversarial Environments**

Maximilian Noppel (Karlsruhe Institute of Technology (KIT)), Christian Wressnegger (Karlsruhe Institute of Technology (KIT))



## **Secure Messaging with Strong Compromise Resilience, Temporal Privacy, and Immediate Decryption**

Cas Cremers (CISPA Helmholtz Center for Information Security), Mang Zhao (CISPA Helmholtz Center for Information Security)



## **Don't Shoot the Messenger: Localization Prevention of Satellite Internet Users**

David Koisser (Technical University Darmstadt), Richard Mitev (Technical University Darmstadt), Marco Chilese (Technical University Darmstadt), Ahmad-Reza Sadeghi (Technical University Darmstadt)



## **SoK: Privacy-Preserving Data Synthesis**

Yuzheng Hu (UIUC), Fan Wu (UIUC), Qinbin Li (National University of Singapore), Yunhui Long (UIUC), Gonzalo Munilla Garrido (Technische Universität München), Chang Ge (University of Minnesota), Bolin Ding (Alibaba Group), David Forsyth (UIUC), Bo Li (UIUC), Dawn Song (UC Berkeley)

